<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Can you make a constraint grammar with only weights?</title>
<!--Generated on Fri May 26 17:12:24 2023 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<link rel="stylesheet" href="../latexml/LaTeXML.css" type="text/css">
<link rel="stylesheet" href="../latexml/ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Can you make a constraint grammar with only weights?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Flammie A Pirinen 
<br class="ltx_break">UiT Norgga árktalaš universitehta 
<br class="ltx_break">Institute of Languages and Cultures / Divvun 
<br class="ltx_break">Tromsø, Norway
<br class="ltx_break"><a href="flammie.pirinen@uit.no" title="" class="ltx_ref ltx_url ltx_font_typewriter">flammie.pirinen@uit.no</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Traditionally, constraint grammar is originated as a system to deal with
ambiguity by means of disallowing and removing readings that are
unacceptable. However, to judge a plausible grammatical parse of a
word-form as definitely unacceptable given only one context condition is
quite risky. What I explore in this experiment is an idea where constraint
grammar’s judgements are not strict but only give relative ordering or
plausibility. My hypotheses are that originally the strict deletion system
of constraint grammar was at least partially motivated by limitations of the
hardware decades ago, that we no longer have to follow. Furthermore, the
construction of parsing system that loops indefinitely on its own input
until the output does not change can be avoided when the weighted constraint
grammar rules are merely given a single reweighting vote type feedback. In
this experiment, I have only implemented a subset of constraint grammar,
also forcing the linguistic description to stick with rules that are either
directly related to a linguistic evidence—giving a likelihood for a
morphological reading because we simultaneously connect it to a syntactic
reading—, or a pure disambiguation preference—when forced to select
between <span class="ltx_text ltx_font_italic">a</span> and <span class="ltx_text ltx_font_italic">b</span>, a is more likely unless specifically
supported by <span class="ltx_text ltx_font_italic">c</span>. I present an experimental usage of the resulting
parsing system as a tool for parsing corpus into dependency treebank that is
post-edited and verified by human.</p>
  
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">One of the most traditional use cases of constraint grammar is based on having
all plausible morphological analyses of word-forms in a sentence as a starting
point, and reducing the ones that are grammatically unsuitable or
unlikely <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="Constraint grammar as a framework for parsing unrestricted text" class="ltx_ref">1</a>]</cite>. However, writing grammatical rules that
express absolute certainties are quite hard to write, and it turns out, that in
many mature constraint grammar descriptions average rules span several maybe
even dozen of lines, with more and more contexts and exceptions added along the
years. This makes the rules sometimes hard to read and maintain. I’ve been
working with such rulesets for several years and my impression is that majority
of the extra exceptions that are being added seem like they are often
exceptional interactions that get copied to several or even most of the rules in
the ruleset, this is one reason why I began experimenting with the idea that
maybe encoding such exceptions as new reweighting rules would make sense.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">I believe one of the reasons of the idea for constraint grammar to operate on
absolute deletion of readings while disambiguating is based on optimisations
which were necessary at the time, a computational requirement that is no longer
as critical as it was. On the other hand, when experimenting with the weighting
constraint grammar logic implementation, I found out I could manage with single
pass over the sentence, since readings are not removed and rules do not feed on
the weights previously used, it can also be quite fast.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Instead of using the existing constraint grammar system I have performed this
experiment with a re-implementation of the rules and grammars in python. I have
limited my re-implementation to a few rule-types that are most common and also
the ones that I find make sense linguistically; one of my design goals in this
experimetn is to write a system that encodes linguistic knowledge into grammars
and their parses and clean up some hackier components of our grammars
altogether. The main functionality in my grammar system is based on
<span class="ltx_text ltx_font_typewriter">SELECT, IFF</span>, and <span class="ltx_text ltx_font_typewriter">REMOVE</span> logics, but instead of operating on
removal of readings, we operate on giving the not-removed reading
<span class="ltx_text ltx_font_italic">weight</span> (penalties<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
            <span class="ltx_tag ltx_tag_note">1</span>
            
            
            
          I use the word <span class="ltx_text ltx_font_italic">weight</span> throughout
the article to mean penalty weight, as is standard in e.g. weighted finite-state
algorithms, I know that in other sub-fields of NLP some extra weight can be a
positive thing, s.t. heavier is better, but in my systems more weight always
means worse.</span></span></span>) The rules I have implemented can have zero or one contexts, if
they have zero contexts (or only context is the same cohort), they are local
disambiguation preferences; I implemented this first because they are one of the
most common types of rules in mature cg rulesets, and most useful in
disambiguation tasks; they are linguistically kind of motivated, since it is
natural to say things like: “if you have to decide between instructive and
genitive, prefer genitive” or “if you have to decide between ‘a dog’ and ‘a
clothes moth’ prefer dog”; I find also linguistically more pleasant to say
than: “instructives (that clash with genitives) <span class="ltx_text ltx_font_bold">do not exist!</span>”, or
“you may <span class="ltx_text ltx_font_bold">never</span> speak about ‘a clothes moth’ (where it clashes with
forms of dog)”. The other form of rule where we have one context is a kind of
linguistic evidence rule; my impression is that most well-formulated rules are
based on linguistic evidence, for this reason I have made them work on
reweighting <span class="ltx_text ltx_font_italic">and</span> dependency drawing principle. This means if you have a
rule of disambiguating or selecting, say accusative reading because there’s a
verb reading in the sentence on the left, it also must mean that, e.g. the
accusative must have an <span class="ltx_text ltx_font_typewriter">obj</span> dependency to that verb (or from, depending
on your dependency grammar preferences).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Some of the potential use cases for this type of grammars are: machine
translation, treebanking, grammar-checking and correction, with human in the
loop, etc. We have performed some initial experiments with dependency
treebanking: the sentences with all possible readings shown to the post-editor
and annotator, they can easily select the highest ranking tree, or remove some
readings, or edit the tree by hand with a text editor in CONLL-u format. In
future experiments I could foresee using the future revisions of the system in
machine translation such that we do not have the problem of early disambiguation
hiding the readings that would be useful or just flat out creating a system that
can produce n-best translations instead of one. For grammar-checking and
correction similarly we currently have specialised grammars that work very
carefully to avoid removing information that will be useful further down in the
pipeline to provide corrections.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib125" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Karlsson</span><span class="ltx_text ltx_bib_year"> (1990)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constraint grammar as a framework for parsing unrestricted text</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 13th International Conference of Computational Linguistics</span>,  <span class="ltx_text ltx_bib_editor">H. Karlgren (Ed.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_place">Helsinki</span>, <span class="ltx_text ltx_bib_pages"> pp. 168–173</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Can you make a constraint grammar with only weights?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 26 17:12:24 2023 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
