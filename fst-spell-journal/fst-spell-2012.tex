\documentclass[a4paper,12pt]{article}

\usepackage{amssymb}
\usepackage{fontspec}

\usepackage{fullpage}
\usepackage{covington}
\usepackage{natbib}

\usepackage{xltxtra}
\usepackage{url}

\usepackage{todonotes}
\usepackage{ifpdf}

\pagestyle{empty}
%\bibliographystyle{natbib.fullname}
\bibliographystyle{cslipubs-natbib}

\title{Weighting Schemes for Language and Error Models in Finite-State
    Spell-Checking and Correction}

\author{Tommi A Pirinen\\
 [0.5cm] University of Helsinki\\ % top level affiliation
 Department of Modern Languages\\ % basic academic or research unit
 \texttt{tommi.pirinen@helsinki.fi}}   % email

\date{\today (draft)}

\begin{document}

\ifpdf
\pdfinfo{
    /Title (Weighting Schemes for Language and Error Models in Finite-State Spell-Checking and Correction)
    /Author (Tommi A Pirinen)
    /CreationDate (D:20120915123456)
    /Subject (Finite-State Spell-Checking)
    /Keywords (FSM;FST;Spell-Checking)
}
\fi

\maketitle 
\thispagestyle{empty}

\begin{abstract}
\noindent 
In this article we \todo{need to rewrite abstract to say something about real
goals} study the finite-state formulations of traditional spelling
correction models. We show different methods for creating probabilistic
finite-state spelling dictionaries for languages of varied morphological
complexity and resources available. We
apply different finite-state weighting methods for the dictionaries and measure
the effects on precision, and speed of the finite-state spelling correction.
Similarly we go through different finite-state formulations of the correction
algorithms as weighted finite-state automata. We show that finite-state
formulation of feature-complete and fast spelling correction is possible for
languages of varying morphological complexity, including those not covered by
traditional spelling correction systems and algorithms.
\end{abstract}

\listoftodos

\section{Introduction} 

The spelling checking and correction is traditional and well-researched part of
computational linguistics' history. Specifically spell-checking and correction
with finite-state automata is one of the more recent branches in effective
spell-checking of typologically varied languages. The finite-state methods of
language models are widely recognised as a good way to handle morphologically
more complex languages~\cite[]{beesley2003finite} in similar manner as
isolating languages as far as recognising correct word-forms is the task. One
of the simplest example of this is that finite-state automata can encode
dictionaries of infinite size, which is relatively common need for languages
with productive derivational and compounding phenomena in morphology.  In these
cases a simple word-list lookup has often proved inefficient. The principal
contribution of this article is to study the use of different weighting schemes
of fully finite-state spell-checking systems for morphologically complex
languages that could not be implemented with the word-list spelling checkers.
We do not provide substantial novel algorithms or weighting techniques but to
survey the existing finite-state models and some non-finite-state models that
can be easily implemented in finite-state form. As the set of languages we have
selected to study Finnish, North Sámi and Greenlandic for the morphologically
complex languages and English to confirm that our finite-state formulations of
traditional spelling correction applications is working as described in the
literature.

Furthermore, as the contemporary spell-checkers are more and more using
statistical approaches to the task, the weighted finite-state models provide
equivalent expressive power even for more complex languages by encoding
probabilities as weights in the automata.  The same applies for error models,
as the programmatic noisy channel models~\cite[]{brill2000improved} can encode
error probabilities when making corrections, so can weighted finite-state
automata encode these probabilities. This article overviews the methods that
are used to encode probabilities into weighted finite-state language and error
models, and evaluates them on larger scale testing material.

As one of the
points of finite-state technology in natural language processing is to bring
the same coverage and quality for morphologically complex language, we also
contrast the results of the most popular commercial spelling correctors with
our finite-state formulations.\todo{why is this blurb here}.

The task of the spell-checking is split in two parts, the error detection and
correction. In the error detection task, the purpose is to locate the words to
be corrected, e.g. to determine that \emph{cta} is not an English word in text
when \emph{cat} is. In literature this is often described as looking up the
words from dictionary or word list, which is nearly accurate for means of this
article as well. In our case though, the word list is substituted with
finite-state automaton, and on more abstract level we refer this as
\emph{language model}, rather than word list, to emphasize the point that it
indeed covers the infinite lexicons of our target languages, and that we can
detect that the word \emph{purppura-apinatiskikone} (purple monkey dish washer)
is a correct word in Finnish language even though you didn't find it in
dictionary nor corpora.  This form of error detection is referred to as
\emph{non-word} and \emph{isolated} error detection. More complex error
detection systems may be used to detect words that are correctly spelled, but
unsuitable in the context based on syntax or semantics, this is referred to as
\emph{real-word} error detection \emph{with context}. While we do not cover
these systems in detail, we refer to \cite{mays/1991} noting that the
implementation of error detection using context-sensitive finite-state system
is plausible based on e.g. approach of~\cite{silfverberg/2010}, which contains
finite-state implementation of n-gram model in reasonable speed. The task of
detecting errors from text is often considered as trivial or solved
\cite[e.g.][]{otero/2007}, so we do not cover that in more detail than what is
needed for tuning weighted finite-state models to perform at least on par to
commonly used spell-checking software.

The task of error-correction, is the task of generating the list of most likely
correct word-forms given the misspelled word-form, that was located in
spell-checking process. The method of correction is often referred to as
\emph{error model}, paralleling language model. This alludes to our practical
implementation that the error correction task is implemented by trying to
simulate the process of making errors as a model. The main point of this error
modeling is to correct the spelling errors accurately by observing the causes
of errors and making predictive models of
these~\cite[]{deorowicz2005correcting}.  This modeling effectively splits the
error models in to numerous sub-categories, each applicable for correcting
specific types of spelling error; the most used and common model is accounting
for mistypings, that is, slips of fingers on keyboard. This model is nearly
language agnostic, although it can be tuned to each local keyboard layout. The
other set of errors is more language specific and user specific, it stems from
lack of knowledge or language competence, e.g.  in non-phonemic orthographies
such as English, learners and unskilled writers commonly make mistakes like
writing \emph{their} instead of \emph{there} since they're pronounced alike;
similarly competence errors will give rise to common confusables in many
languages, like missing an accent, writing some digraph instead of its unigraph
variant, or confusing some morph with another.

There are two main approaches applied in this article for the task of ranking
the correction suggestions; using the probabilities given by the dictionary for
the word-forms that are correct, and using the probabilities given by the
error-correcting model for the likelihood of user typing the specific
misspelling when meaning the corrected form, i.e. likelihood of user making
the specific series of errors. This article surveys the existing
weighted finite-state methods for creating these two probabilistic models and
combining them. One note should be made when thinking of error correction,
unlike some articles claim, the application of error model is not dependent
of the type of the errors detected, and these two components can be kept fully
apart; an error model that corrects non-word errors will come up with (possibly
suboptimal) suggestions, and error model tuned for real word errors will
correct non-word errors.

The another source of probabilities that is commonly used
\cite[]{pirinen2012improving,otero/2007} for ranking the suggestion is the
neighboring words and word-forms. This weighting scheme is not thoroughly
evaluated in this article, but we show how it can be trivially implemented in
conjunction with the context agnostic methods.

One of the recent themes in research of finite-state spell-checking and
correction is the aspect of using weighted finite-state automata to improve the
precision or accuracy of the spelling correction, and even the checking,
methods. What is unfortunate is that these methods often require large amounts
of data; to train the language models one would expect to have corpora where at
least majority of correctly spelled word-forms available. Even if polysynthetic
languages, like Greenlandic, would have freely available gigaword corpora,
which they don't, it might not be nearly as complete as English corpus with
million word-forms. To cope with resource-poor morphologically complex
languages in situations like this, we study few of the more advanced
linguistically motivated corpus training methods, like compound
part~\cite[]{pirinen/2009/nodalida} or morpheme weighting schemes and study
their effects.

Another aspect with resource-poor languages is that more advanced language
model training schemes, such as use of morphological analyses as error
detection evidence~\cite[]{mays/1991} and factor in correction
ranking~\cite[]{otero/2007}, would require large manually verified
morphologically analysed corpora, which simply do not exist as open, freely
usable resources. Same problem in smaller scale applies to improving error
correction models with data from error corpora; the corpus must contain both
the errors and right corrections to it. For this reason we concentrate here in
showing what kind of improvement can be made with tiny, easily doable corpora
of this kind, accompanied with hand-crafted rules or guesstimated
\emph{probabilities}.

This article is structured as follows: In following
subsection~\ref{subsec:background} we describe the history of spell-checking up
to finite-state formulation of the problem, then show traditional finite-state
approaches to the problem, and describe how our weighted finite-state
implementation compares to the other applications presented in the literature.
In subsection~\ref{subsec:theory} we briefly revisit the notations and
assumptions behind quasi-statistics we apply to our language and error models.
In section~\ref{sec:methods} we show the popular methods of creating
finite-state language and error models for spell-checkers, describe getting
probabilities or similar weights into the finite-state spell-checker, describe
methods of inducing weights into language and error models, creating weighted
language and error models from data, and finally the methods to combine the
weights of different sources of data and probabilities. In~\ref{sec:material}
we then present the actual data, existing language models, error models and
corpora we have used, and in~\ref{sec:evaluation} we show how the different
combinations of languages, weighing schemes and error models affect to accuracy
and precision, and speed of the finite-state spell-checking.

\subsection{Brief History of Automatic Spell-Checking and Correction}
\label{subsec:background}

The automatic spelling correction by computer itself is an old invention, with
main work done as early as in 1960's, such as invention of the ubiquitous error
model for typing mistakes, the Levenshtein-Damerau distance
\cite[]{levenshtein/1966,damerau/1964} and first applications of the noisy
channel model~\cite[]{shannon/1948} for spell-checking~\cite[]{raviv/1967}.
The initial solutions treated dictionaries as simple word lists, or later,
word-lists with up to a few affixes with simple stem mutations and basic
compounding processes. The most recent and widely spread implementation of this
dictionary consisting word-list, stem mutations, affixes and some compounding
is hunspell\footnote{\url{http://hfst.sf.net}}, which is still rather
ubiquitous in open source world of spell-checking and correction, and is one of
our reference implementation when we describe the methods of contemporary
spelling checkers in section~\ref{sec:methods}. The word list approach even
with some affix stripping and stem mutations has usually been found
insufficient for morphologically complex languages.  E.g. even recent attempts
to utilise hunspell for Finnish have not been successful
\cite[]{pitkanen/2006}. And in part the popularity of finite-state methods in
computational linguistics seen in the 1980's was driven by need for
morphologically more complex languages to get the language models and
morphological analysers with recurring derivation and compounding processes
\cite[]{beesley2004morphological}.  In this light it is almost ironic that many
of the recent approaches for finite-state spell-checking are still concentrated
on using acyclic finite-state automata (i.e. equivalent of word list) to
perform spell-checking~\cite[]{watson2003new,deorowicz2005correcting}, while
this approach has the additional performance, the possibility to use arbitrary
finite-state automata as language models comes without any measurable
modifications to the code~\cite[e.g.][]{pirinen/2010/lrec} and leaves the task
of optimising to the writer of the dictionary.

Given finite-state representation of the dictionaries and the expressive power
of finite-state systems, the concept of finite-state based implementation for
the spelling correction was obvious development. The earliest approaches
presented algorithmic ways to implement finite-state network traversal with
error-tolerance \cite[]{oflazer/1996} in fast and effective manner
\cite[]{agata/2002,hulden/2009}.  In \cite{schulz/2002} the Levenshtein-Damerau
distance was presented in finite-state form such that the finite-state spelling
correction could be performed using standard finite-state algebraic operations
with any existing finite-state library. Furthermore in e.g.
\cite{pirinen/2010/lrec} it has been showed that weighted finite-state methods
can be easily used to gain same expressive power as existing spell-checking
software algorithms.

\subsection{Notations and a Bit of Statistics for Language and Error Models}
\label{subsec:theory}

In this article, where formulas of finite-state algebra are concerned, we
assume the standard notations from \cite{aho2007compilers}: a one-tape
finite-state automaton $M$ is a system $(Q, \Sigma, \delta, Q_s, Q_f, W)$,
where $Q$ is the set of states, $\Sigma$ the set of alphabet, $\delta$ the
transition mapping of form $Q \times \Sigma^t \rightarrow Q$, where t is number
of tapes in automaton, $Q_s$ the initial states of the automaton and $Q_f$ the
final states of the automaton. For weighted automata we extend as in
\cite{mohri2009weighted} such that $\delta$ is extended to $Q \times \Sigma^t
\times W \rightarrow Q$, where $W$ is the weight, and additionally system
includes final weight mapping $\rho: Q_f \rightarrow W$. The structure we
use for weights is systematically the tropical semiring 
$(\mathbb{R}_+ \cup {+\infty}, min, +, +\infty, 0)$, i.e. weights are positive
floats that are collected by addition.

For finite-state spell-checking we use following common notations: $M_d$ is a
single tape weighted finite-state automaton used for detecting spelling errors,
where threshold $w$ may be used to discount bad word forms, $M_s$ is a single
tape weighted finite-state automaton used for suggesting correct words, where
weight value is used to rank the suggestions. In many occasions we study the
possibility of $M_d = M_s$. 

In where probabilities are used, the basic formula to get probabilities from
discrete frequencies of events (word-forms, mistyping events, etc.) is
straightforward $P(x) = \frac{c(x)}{\mathrm{corpus size}}$, where x is the
event, c is the count or frequency of the event, and corpus size is sum of all
event counts in the training corpus. The encoding to weight structure of
finite-state automaton is done by setting $Q_{\pi_x} = -\log P(x) \in \rho$.
As events not appearing in corpora should not in some cases have probability of
zero, we use either simple additive smoothing techniques, setting $P(x) =
\frac{c(x) + \alpha}{\mathrm{corpus size} \times (1 + \alpha)}$, so for unknown
event $\hat{x}$ the probability will be counted as if it had $\alpha$
appearances.  Another approach is to set $P(\hat{x}) < \frac{1}{\mathrm{corpus
size}}$, which makes probability distribution leak but may work under some
conditions \cite[]{brants2007large}.

\section{Contemporary Methods for Making Finite-State Language and Error Models
and Their Weighting}
\label{sec:methods}

The task of spell-checking is divided into locating the spelling errors and
suggesting the corrections for the spelling errors. In finite-state
spell-checking the former task requires a language model, that can tell whether
or not given string is correct. Simplest finite-state approach for this is just
an unweighted single-tape finite-state automaton where all strings recognised
by the automaton are considered correct. It is also possible to use two-tape
automata where information encoded on second level aids in deciding whether the
string should be accepted under current settings (e.g. with offensive or sub
standard language), and also weighted finite-state automaton with threshold of
considering very rare or unusual words as errors---this approach is usually
taken with context-sensitive applications~\cite[]{otero/2007}. The error
correction requires language model, which may or may not be the same as with
error detection, and an error model. The language model for spelling
correction, like the one for error detection, is in most simple case just an
unweighted finite-state automaton encoding correct strings of language. In case
of correction however, even very simple probabilistic weighting from small
corpora of unverified texts will improve the quality of
suggestions~\cite[]{pirinen/2010/lrec}, so having it weighted is usually a good
thing. Another difference between spell-checking language model and correction
model in many practical applications is, that typically the checker can be much
more permissive with offensive words, unlikely derivations, and compounds, it
is usually considered bad to have spell-checker suggest these word-forms as
corrections. It is likely that with probabilistic data these forms will be
discounted to end of suggestion list any ways, but some users will often prefer
not to see them at all. The error modeling part of the error correction is made
with a two-tape finite-state automata, that can encode the relation of from
misspellings to the correctly typed words. This relation can also be weighted
with probabilities of making specific mistypings or errors or just arbitrary
penalties, as is basically done with many traditional software-based approaches
\cite[such as][]{hunspell/manual}. It should be noted that this error model
automaton can basically be a collection of one to many different strategies or
algorithms that each model spelling or typing errors in their own way.

This chapter is organised as follows, in \ref{subsec:language-models} we
describe how finite-state language models are made and how they can be tuned
for spell-checking use, as well as the basic probabilistic and hand-written
weighting techniques that have been used to implement weighted language models
in finite-state context. In \ref{subsec:error-models} we go through number of
popular schemes for modeling typos and other spelling errors. 
In~\ref{subsec:manual-weighting} we go through some finite-state weighting
schemes that are based on mainly expert judgement and fiddling of the weights,
most of which are aimed at recreating results of other spelling checkers and
correctors. In~\ref{subsec:automatic-weighting} we study methods for
inducing the weights for language and error models from unannotated and
small annotated corpora, and in~\ref{subsec:combining-weights} we show both
statistically sound and \emph{stupid} methods of combining the weights in the
models.

\subsection{Compiling Finite-State Language Models}
\label{subsec:language-models}

The absolute baseline for the language model as realised by numerous
spell-checking systems and lots of literature is a word-list (or word-form
list) of some kind or another. One of the most popular example of this approach
is \cite[]{norvig/2010} describing programming a spelling corrector over a time
of an intercontinental flight. The finite-state formulation of this idea is
equally simple; given a list of word-forms we compile each string as a path in
the automaton \cite[]{pirinen2012effects}. In fact, even the classical
optimised data structures used to efficiently encode word lists, like tries and
acyclic deterministic finite-state automata are usable as finite-state automata
for our purposes without modifications.

Moving to more advanced word lists, such as affix stripping and stem mutating
ones of hunspell, the finite-state formulation comes slightly more complex, but
the basics are same: the roots are disjunction of string paths and so are the
affixes. The correct morphotactic combinations and stem mutations they inflict
need to be calculated out when constructing the automaton, but this can be
easily done with e.g. set of parallel restraints encoded in intersection of
two-level automata \cite[]{pirinen2010creating} in vein of two level
morphology.

The main purpose of finite-state spell-checking of course is to get efficient
spell-checking to the languages that could not have been made with
above-mentioned non-finite-state methods. These are approaches such as the
original two-level morphology~\cite[]{koskenniemi/1983} or the development on
it by xerox in \emph{Finite-State Morphology}~\cite[]{beesley2003finite}.  More
over, this also includes recent open-source systems for natural language
processing based on finite-state technology, such as rule-based
machine-translation system apertium~\cite[]{apertium2010}, and finite-state
formalisms like sfst~\cite[]{schmid2006programming} and
kleene~\cite[]{beesley2012kleene}.  The language models these systems produce
are of course all finite-state automata that can be attached to spell-checking
system with very little effort~\cite[e.g.][]{pirinen2012compiling}.

\subsection{Compiling Finite-State Versions of Error Models}
\label{subsec:error-models}

The baseline error model for spell-checking is most conveniently the
Damerau-Levenshtein distance measure. As The finite-state formulations of the
error models are the most recent development in finite-state spell-checking,
the earliest reference to finite-state error model in actual spell-checking
system is by \cite{schulz/2002}, it also contains very thorough description of
building finite-state models for different forms of edit distances. The basic
idea is this: for each type of error: insertion, deletion and changing of
letters, add one arc $x:\epsilon$, $\epsilon:x$ and $x:y$ respectively, for
each alphabet $x, y \in \Sigma, x \neq y$ from the initial state to the final
state (this is 2 state automaton). To extend this with swaps of adjacent
characters, we have to reserve one state from the automata for each character
pair $x:y$, such that following $y:x$ will lead to the final state
\cite[]{pirinen/2010/lrec}\footnote{with the modification that each state apart
from start state are final states, apparent omission in their formulation}.
The fixed automaton for mock alphabet $\Sigma = {x, y} \cup {?}$, where $?$
denotes any unknown symbol\footnote{This extension is relatively common for
    finite-state methods in natural language processing, its full
    implementation in finite-state systems is not entirely trivial and not
    well-documented, but we refer the reader to \cite[]{beesley2003finite} for
    details on one implementation of it.}, is given in
    figure~\ref{fig:xy-edit-1}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{graphicx/xy-edit1}
    \caption{Damerau-Levenshtein edit distance 1 based error model for
        alphabet {x, y, ?}
    \label{fig:xy-edit-1}}
\end{figure}

One popular modification to speed up the edit distance algorithm is to disable
the modifications at first character of the word, this provides a big
improvement since word-length is a factor in speed of correction generation,
and the selection of first character is based on some
data~\cite[]{bhagat2007spelling}.  This simple modification provides measurable
speedup at cost of recall. The finite-state implementation of it is simple, we
concatenate one unmodified character or $?$ symbol in front of the error model.

The phonemic folding schemes obviously depend from language to language, but
basic idea of them is usually the same: assign some value to set of similarly
sounding parts of words; these can be as simple as context-independent mappings
or as complex as hundreds of parallel rules with contexts. Here we introduce a
finite-state formulation of the soundex algorithm \cite{russell1918soundex},
originally made for cataloguing English language names. The soundex algorithm
is quite simple. It assigns each word to its first letter, then three number
sequence mapping the letters that are concidered phonemically important to up
to three numbers\footnote{slightly modified from
\url{http://en.Wikipedia.org/wiki/Soundex} to match the finite-state formula}:

\begin{enumerate}
    \item Retain the first letter of the name 
    \item Replace following letters like so:\begin{itemize}
            \item drop all other occurrences of a, e, i, o, u, y, h, w, ?
            \item b, f, p, v becomes 1
            \item c, g, j, k, q, s, x, z becomes 2
            \item d, t becomes 3
            \item l becomes 4
            \item m, n becomes 5
            \item r becomes 6
        \end{itemize}
    \item Except following:\begin{itemize}
            \item Two adjacent letters with the same number are coded as a
                single number
            \item also two letters with the same number separated by 'h' or 'w'
                are coded as a single number,
            \item whereas such letters separated by a vowel are coded twice.
            \item This rule also applies to the first letter.
        \end{itemize}
    \item If the resulting sequence has less than three digits, fill in with
        zeroes.
\end{enumerate}

Now the finite-state version is rather obvious, for actual automaton, see
appendix \ref{appendix:soundex}. The first rule is encoded by simply going from
start state to specific states for each six letters that might need to be
skipped when doubled, or seventh state for letters that do not correspond to
number, then each of these states skips the skippables or encodes numbers as
laid out in the second rule. Ther resulting automaton is capable of turning
words into soundex codes, such as \emph{Levenshtein:L152}. In order to use this
as an error model we need to be able to map L152 back to all possible words
corresponding string L152 (there are infinitely many); in finite-state
technology this is as simple as composing the mapping with its inversion. Along
the years more elaborate algorithms have been developed for English, such as
speedcop, and metaphone in three
incarnations\cite{philips1990hanging,philips2000double}\footnote{the third
version, Metaphone 3, is a commercial product that has not been openly
documented and cannot be used in free/open source or academic research
systems}, mainly what they do is add more rules, maybe more numbers or letters
for folding, but implementation is the same and finite-state formulation
likewise. For languages other than English there are fewer of these, since many
of languages are written in more phonemic orthographies and this error type is
non-existent, or handled with more simple models, like with all our other
experimentation languages.

The errors that do not come from typing errors nor phonemic misunderstandings
are nearly always covered with specific string transformations---or even
relying on the edit distance algorithm. Encoding simple string transformations
as finite-state automata is very trivial; for any given transformation $S:U$ we
have a path $\pi = S_1:U_1 S_2:U_2 \ldots S_n:U_n$, where n is $max(|S|, |U|)$
and missing characters of shorter word substituted with epsilons. Obviously the
path can be extended with arbitrary contexts $L _ R$ by concatenating those
contexts on left and right respectively. 

Concerning the current de-facto standard of open source spell-checking, 
hunspell, the suggestion approaches are variations of what is described earlier
in this chapter:

\begin{itemize}
    \item[KEY] is keyboard layout adjusted single replacement type of spelling
        error for specific subsets of alphabet, e.g. rows and columns of a
        keyboard
    \item[TRY] is edit distance algorithm without swaps or replacements
    \item[REP] is for confusable substrings within words
    \item[MAP] is single character to n characters REP with higher priority
    \item[PHONE] is phonemic folding with specific tables using double
        metaphone
\end{itemize}

Each of these error models have different priorities in hunspell, implemented
in program code as sequential processing. The finite-state implementation
encodes the priorities as weights, as detailed in
\ref{subsec:manual-weighting}.

One of the obvious features of using regular finite-state automata as models of
errors, we can turn from edit distance 1 into an edit distance n algorithm by
repetition or composition operation of finite-state
algebra\cite{pirinen2012effects}.  Similarly combining various error models
together into one finite-state automaton is performed simply by unions (for
full-string error models) and
concatenations (for word internal error models).

\subsection{Manually Weighting Error and Language Models Using Expert
Judgement}
\label{subsec:manual-weighting}

The manual weighting schemes for finite-state systems have few common
rationales: the most common one is that there are no suitable corpora to
automatically get weights from. For language models it has been shown that even
smaller Wikipedia's with modest quality of texts in terms of grammatically
correct texts will provide improvement~\cite{pirinen/2010/lrec}, however for
more advanced weighting, such as morphological feature-based
one~\cite{pirinen2012improving} or the error-model weighting, the requirement
is already at large amounts of manually annotated high quality texts. The
manual weighting schemes are in effect similar as software based approaches
which arrange suggestions in order based on e.g. flags in dictionary; we
merely replace flags with weights in the paths.

One of the most basic ranking schemes of word-forms that has been used in
morphosyntactic analysers of morphologically more complex languages is that
morphologically simpler word-forms are more
likely~\cite{karlsson1992swetwol}; this means that we should suggest simple
words before derivations or compounds. In weighted finite-state form this
restriction is attaching weight to compound and derivation boundaries or
analyses, which are encoded in the dictionary. In hunspell this corresponds to
setting MAXCPDSUGS. Similar consideration can be made for other features that
may be available or encodable in the language model: giving more weight to
rare form suffixes, rare lexemes or substandard forms.

Designing the weighting scheme manually for morphologically complex languages
is quite simple, since we use tropical semiring and its collect operator is
regular addition. For example for Finnish we might want to say that word with
instructive suffix is less likely than four-part compound, we assign
instructive a weight that is equal or greater than four times the weight of the
compound boundary (because in Finnish instructive suffix can only appear once
per word-form we can know it doesn't stack, whereas compounding is productive
and unlimited).

For error model part we perform similar considerations, for example following
the Hunspell model of error corrections, we start by allowing the most
specific errors to be most likely and assign smallest weight or zero weight
to them; this applies to commonly confused word-forms and substrings (the REP
setting). The edit distance measures can be weighted manually with few
approaches: the characters adjacent to each other on keyboard are likely
for substitution error (the KEY setting) whereas others may have language
specific considerations (e.g. in spoken Finnish some of the vowels are commonly
dropped so it may be useful to give insertion of vowels less weight to bring
those suggestions in front of the list). As Hunspell will arrange these
approaches such that first REP suggestions then KEY then TRY, to simulate this
we would assign REPs half or less than KEYs' (substitutions) weights and
KEYs half or less of TRYs weight (insertions, deletions and swaps).

\subsection{Automatic Weighting of Error and Language Models from Corpus Data}
\label{subsec:automatic-weighting}

Assuming we have suitable corpora for training the language and error models,
or even inducing the whole models from the corpora alone, we can use simple
scripts to create the models from the data, and to include them in existing
models. One of the reasons why this is very tempting approach, is that it will
give improvements with even relatively small corpora and rarely decrease the
quality or even speed. The reason for this is simple; the unweighted language
model acts as if all word-forms have equal probability and the suggestions of
same distance will be generated in arbitrary order (e.g. alphabetical). Any new
probability will just act as if it was the unweighted model, but in addition
the strings that were in training corpora are slightly preferred in order of
popularity, similarly for error corpora and models, this will at least make the
ordering more arbitrary, and unless the earlier arbitrary order was more
favorable systematically, the result improves. Specifically it does not suffer
as much from sparseness of data as other tasks like statistical machine
translation or morphological analysis where one unseen event will accumulate
over the course of sentence or so. Also on assumption that typos are events
that happen at fairly regular distribution the results will follow that
distribution for any reasonable corpus material. Finally, even if mistakes of
some user do not follow standard distribution, they are more understanding
towards spell-checker that suggests common words with common typing errors than
ones that suggest very rare and obscure word-forms.\todo{back this up?}

The most simple language model for spell-checking to induct or learn, is
the surface word-form unigram probability model. This model is simply created
as finite-state automaton by taking all strings from the corpora along with
their frequency-based weights, and disjuncting them into a simple acyclic
finite-state automaton, where each string now corresponds to a path and the end
weight of the path is the probability cast into tropical weight. One easily
acquirable model like this is Wikipedia~\cite{pirinen/2010/lrec}, which has at
least some data even in many of the lesser resourced languages.

As many corpora like Wikipedia do not give totally good coverage of correctly
spelt standard language, it may be useful to consider pruning the least likely
forms either during compilation or by using thresholds in the spell-checking
and correction phase. A better approach to ensure that spell-checker only
allows and suggests normative good language is to create the language model by
hand, and then weight it with the corpus data afterwards, only counting strings
that are found in the language model into the weighting scheme
\cite{pirinen/2009/nodalida}. Since most of our language models from
morphologically more complex languages contain infinite amount of word forms,
usually the corpus will not contain all of them, and the most basic weighting
scheme would set the final weight of these paths to infinite for likelihood
estimate of 0, making them not accepted. There are few considerations to be
done with this; for language like Finnish or German \cite{schiller2006german}
it is possible to weight compounds by weighting the separate word form parts in
the compound, possible further penalising the compounds still if needed---this
approach seemingly breaks the statistical well-formedness of the structure, but
is found to work rather well; this is partially in line with other results on
statistical natural language processing in \cite{brants2007large}. The
schoolbook approach for the problem is by offsetting the part of probability
mass with smoothing algorithm when counting the probabilities of the word form,
then distributing the mass along the unseen word-forms\cite[for a good
introduction to smoothing models we refer
to][]{jurafsky2000speech}; depending on model we either apply simple
additive smoothing as it is cheap to implement and works well enough for these
experiments; for measure of how smoothing method affects quality see e.g.
\cite{chen1999empirical}. In finite-state form this is done by subtracting the
seen words from the language model\footnote{for optimisation this part may be
    omitted; even penalising the whole language model will only leave duplicate
    paths that have large weights and will not have effect to the results,
    while counting the subtraction may often be very resource heavy.}, and
    composing the penalty weight to all end states of the resulting automaton
    with a weighted universal language automaton.

Training the error model we need to have a corpus of spelling errors---possibly
with correct forms attached. The basic theory for this for non-finite-state
form is taken from \cite{church1991probability}, which learns the weights of
the standard edit distance model by simply picking words that are not in
language, correcting them with single edit distance model, and counting the
specific errors iteratively, in this approach the errors are learnt by just
seeing potential errors in the corpora, without knowledge of whether they are
the right errors that were made in the text.

Now each edit distance error arc of form $x:y$, $x, y \in \Sigma \cup
{\epsilon}$ in the error model is to be weighted by the $-log
\frac{c(x:y)}{\mathrm{error count}}$, $c(x:y)$ is the count of x:y corrections
in the corpus. In \cite{brill2000improved} it is proposed that edit distance be
replaced by arbitrary string-to-string mappings; this extension is possible to
Church's method of error corpus creation and existing error model of string to
string mappings. To collect these models we modified the spell-checking
algorithm to emit the string of edits along with corrections when printing the
result paths of composition from misspelt string, the error model and the
language model (i.e. print the path in error model traversed that gets
clobbered in composition) similarly as is done in \cite{ristad1998learning}.
This allows us to collect both full positional string-to-string frequencies as
well as single edit frequencies.\todo{there's no proper implementation yet}

\subsection{Combining Weights from Different Sources and Different Models}
\label{subsec:combining-weights}

Since both our language and error models are weighted, the weights need to be
combined when applying the error and language models to misspelt string. Since
the application performs what is basically a finite-state composition, the
default outcome is a weight semiring multiplication of the values, this is an
real number addition in the tropical semiring. With the basic assumption of
automatic weighting schemes, that the weights are probabilities, this is equal
to standard multiplication of the probabilities. Since we can assume the
probabilities are independent \cite[]{church1991probability}, this is a
reasonable way to combine them, which can be used for a good baseline. In many
cases, however, it is preferable to treat probabilities or weights drawn from
different sources as unequal in strength. For example in many of the existing
spelling-checker systems, it is preferred to first suggest all corrections that
contain only one spelling error before the ones with two errors, regardless of
the likelihood of the word forms in language model. To accomplish this, we have
to scale the weights in the error model to ensure that any weight in error
model is greater than or equal to any weight in the language model, this can be
accomplished with for example following simple formula: $w_e
\mathrel{\mathop:}= w_e + max(w_l | \forall w_l \in W_l)$.

\section{Source Data for Typologically Varied Test Cases for Finite-State
Spell-Checking}
\label{sec:material}

To evaluate the weighting schemes, language and error models we have selected
three of the morphologically more complex languages with small to virtually no
corpus resources available: Finnish, North Sámi and Greenlandic. The
comparative baseline from large, morphologically simple language we use English
for that, since we can simultaneously use it to recreate the results of
reference literature, at least whenever the cited works properly distribute
their data for recreational purposes. The prose in this section briefly
introduces the data and methods to compile the models in informative manner;
for exact implementation, repeatability of result, or trying to implement same
approaches for another language, reader is advised to utilise the scripts,
programs and makefiles available from our source code
repository\footnote{}.\todo{url contains version history so it's not anonymous
by any measure. also, new or old sf repo?}

For language model of English we use data from
\cite{norvig/2010,pirinen2012effects}, this is a basic statistical language
model based on a frequency weighted word-list extracted from freely available
Internet corpora (Wikipedia, project gutenberg). As the word list here is
collected from sources that are not entirely normative language and style, we
also implement language models with rare word-forms excluded. The error models
for English are combined from basic edit distance with English alphabet a-z,
the soundex algorithm, the confusion set from hunspell's English dictionary
containing 96 REP confusion pairs\footnote{}.\todo{url to en-us.aff}

The language models for Finnish, North Sámi and Greenlandic are drawn from the
free/libre open source repository of finite-state language models managed by
university of Tromsø\footnote{\url{http://giellatekno.uit.no/}}.\todo{or divvun
or both} The models are all based on morphological analysers built in
finite-state morphology \cite{beesley2003finite} fashion. This repository also
includes basic versions of finite-state spell-checking under the same framework
that we use in this article for testing. The error models for languages use all
the edit distance models based on each language's alphabet. To compile
our dictionaries, we have used the makefiles available in the repository.
For coverage tests we have made acyclic versions of the morphological
analysers by locating circularities in the paths of the implementations
and disallowing the elements that mark circular paths, these elements are
available in the morphological analysers such as \texttt{+Use/Circ}, 
\texttt{+Guess}, \texttt{+Deriv/} and \texttt{+Cmp/}. To cut paths we use
simple composition of term complement's Kleene star closure.

The training corpora for each of languages is based on Wikipedia; to train the
language model we have used the correct word-forms of first 80~\% of the
Wikipedia, the nonwords for error model, and the off-set 20~\% is used to test
the models. For another experiment we also device a language model for these
languages solely from the corpus training data, as with English.  For English
we select only the 
initial 20 \% of the corpus to train the model and next 5~\% to test
\todo{check figure after implementing} it---this corresponds roughly the same
size as Finnish test case and several orders of magnitude greater than North
Sámi or Greenlandic. In order to verify that there's no over-fitting of the
models, we have acquired Birkbeck corpus for English, and few smaller handmade
error corpora for Finnish and North Sámi from students'
texts\footnote{regrettably we cannot release these corpora in accordance to
privacy laws relevant to such writings}.

\section{The Speed and Quality of Different Finite-State Models and Weighting
Schemes}
\label{sec:evaluation}

To evaluate the systems, we have used a modified version of HFST spell-checking
tool \texttt{hfst-ospell 0.2.2} found in the repository under 
\footnote{\url{}}\todo{new sf.net urls? anonymised?} with otherwise default
options, but for speed measurements we have used \texttt{--profile} argument.
The evaluations on speed and memory use have been performed by averaging over
three test runs on an dedicated high end server: \ldots, except for the
black-box measurements of commercial spelling-checkers, which have been done by
hand on \ldots.\todo{check the specs for hfst server and windows boxen}

To measure the quality of the spell-checking we have run the list of misspelled
words through a spelling our spelling correctors, extracting all suggestions.
The quality is measured by proportion of correct suggestions for first five
positions cumulatively, and the percentage for rest of the positions. So, the
first column of tables is $\frac{c(\mathrm{correct ranked 1})}{c(errors)}$ and
second column is $\frac{c(\mathrm{correct ranked 1} + c(\mathrm{correct ranked
2})}{c(errors)}$ and so forth. In the error analysis
subsection~\ref{subsec:error-analysis} we note all words that are not in the
suggestion list at all, it is, when error model is too weak to produce expected
correction; these are same numbers regardless of weighting.

To evaluate the very first baseline, we measure the coverage of our language
models, and coverage of language models + error models, that is, we measure,
how much of the texts themselves can be matched at all, using just the language
models themselves and the error models, and how many of the word-forms are
beyond the reach of the models. In this measurement we also show how badly the
pure corpus based language models and language models that are forced acyclic
by cutting all cycles fare for morphologically more complex languages, since
we've often heard comments to the effect that we have not substantiated the
need of using cyclic finite-state language models for such languages.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \bf Errors: & \bf 0  & \bf 1 & \bf 2 & \bf 3 & \bf 4 & \bf 5 \\
        \bf Language and error models &  \% & \% & \% & \% & \% & \% \\
        \hline
        \bf English Norvig & 80.1 & & & & & \\
            \bf English WP & 98.7 & 98.8 & & & & \\
        \hline
                   \bf Finnish WP & & & & & & \\
                  \bf Finnish AFSA & & & & & & \\
                  \bf Finnish FSA & & & & & & \\
        \hline
        \bf North Sámi WP & & & & & & \\
               \bf North Sámi AFSA & & & & & & \\
               \bf North Sámi FSA & & & & & & \\
        \hline
        \bf Greenlandic WP & & & & & & \\
                 \bf Greenlandic AFSA & & & & & & \\
                  \bf Greenlandic FSA & & & & & & \\
        \hline
    \end{tabular}
    \caption{The coverage of basic language and error models without weighting
        or measurement of quality.\label{table:baseline-coverage}}
\end{table}

To give an impression of the spell-checking task's complexity in terms of time
and space requirements for the suggestion generation part, we show in figure
\ref{fig:coverage} the frequencies of numbers of possible suggestions as function of error
and language models.

\begin{figure}
    \centering
    \missingfigure{The histograms}
    \caption{A plot of dubious quality
    \label{fig:coverage}}
\end{figure}

\subsection{Quality Evaluations}


In table \ref{table:baseline-quality} 
\todo{Fill all the tables amirite}
we set the baselines for using the
language and error models without any special weighting systems, this means
that the error models have homogenous weights per error greater than language
models weights, which are determined by the simple $-\log P(w)$ formula.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \bf Rank: & $1^{st}$ & $2^{nd}$ & $3^{rd}$ & $4^{th}$ & $5^{th}$ & rest \\
        \bf Language and error models &  \% & \% & \% & \% & \% & \% \\
        \hline
        \bf English w/ 2 edits & & & & & & \\
     \bf - 2 edits and soundex & & & & & & \\
   \bf - 2 edits and conf. set & & & & & & \\
          \bf - hunspell style & & & & & & \\
        \hline
        \bf Finnish w/ 2 edits & & & & & \\
        \hline
        \bf North Sámi w/ 2 edits & & & & & \\
        \hline
        \bf Greenlandic w/ edits & & & & & \\
        \hline
    \end{tabular}
    \caption{The quality comparison of basic model combinations without special
    weighting schemes\label{table:baseline-quality}}
\end{table}

One of the popular optimisations for error models is to vary the edit distance
length, or more generally, number of times errors in error model are applied.
Another commonly used one is to disallow the modifications of the first
letter of the word. Both of these are obvious speed versus quality
trade-offs, in table \ref{table:optimisation-quality} we measure the effect of
simple extension of the error model by repeating it to quality of the results.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \bf Rank: & $1^{st}$ & $2^{nd}$ & $3^{rd}$ & $4^{th}$ & $5^{th}$ & rest \\
        \bf Language and error models &  \% & \% & \% & \% & \% & \% \\
        \hline
        \bf English w/ 1 error & & & & & & \\
     \bf English w/ 2 errors & & & & & & \\
   \bf English w/ 3 errors & & & & & & \\
          \bf English w/ 4 errors & & & & & & \\
 \bf English /w 1 non-first error & & & & & & \\
 \bf English w/ 2 nonfirst errors & & & & & & \\
 \bf English w/ 4 nonfirst errors & & & & & & \\
        \hline
        \bf Finnish w/ 1 errors & & & & & & \\
        \bf Finnish w/ 2 errors & & & & & & \\
\bf Finnish w/ 1 nonfirst error & & & & & & \\
        \bf Finnish w/ 2 nonfirst errors & & & & & & \\
        \hline
        \bf North Sámi w/ 1 error & & & & & & \\
        \bf Nort Sámi w/ 2 errors & & & & & & \\
        \bf North Sámi w/ 1 nonfirst errors & & & & & & \\
        \bf North Sámi w/ 2 nonfirst errors & & & & & & \\
        \hline
        \bf Greenlandic w/ 1 error & & & & & & \\
       \bf Greenlandic w/ 2 errors & & & & & & \\
        \bf Greenlandic w/ 1 nonfirst error & & & & & & \\
       \bf Greenlandic w/ 2 nonfirst errors & & & & & & \\
        \hline
    \end{tabular}
    \caption{The quality comparison of basic model combinations without special
    weighting schemes\label{table:distance-quality}}
\end{table}

Finally we compare the results of our system to the actual systems in everyday
use, that is, hunspell for open source solutions including openoffice and the
kind, and Microsoft's Word for commercial companies. For this comparison we
have only used a subset of 

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \bf Rank: & $1^{st}$ & $2^{nd}$ & $3^{rd}$ & $4^{th}$ & $5^{th}$ & rest \\
        \bf Language and error models &  \% & \% & \% & \% & \% & \% \\
        \hline
        \bf English w/ 2 edits & & & & & & \\
     \bf - 2 edits and soundex & & & & & & \\
   \bf - 2 edits and conf. set & & & & & & \\
          \bf - hunspell style & & & & & & \\
        \hline
        \bf Finnish w/ 2 edits & & & & & \\
        \hline
        \bf North Sámi w/ 2 edits & & & & & \\
        \hline
        \bf Greenlandic w/ edits & & & & & \\
        \hline
    \end{tabular}
    \caption{The quality comparison of basic model combinations without special
    weighting schemes\label{table:commercial-quality}}
\end{table}

To summarize the results we provide a neat graph here:

\begin{figure}
    \centering
    \missingfigure{This is gonna be a cool R plot}
    \caption{A plot of dubious quality
    \label{fig:quality}}
\end{figure}


\subsection{Speed Evaluation}

For practical spell-checking systems there are multiple levels of speed
requirements, so we measure the effects of our different models to speed to
see if the optimal models can actually be used in interactive systems, offline
corrections or just batch processing. In table~\ref{table:speed} we show the
speed of different model combinations for spell-checking. For more thorough
evaluation of the speed of finite-state language and error models we refer to
\cite{pirinen2012improving}.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r||r|r|r|}
        \hline
        \bf Input: & 1 & 1,000 & 1,000,000 & 1 & 1,000 & 1,000,000 \\
        \bf Language and error models & s & s & s & s & s & s \\
        \hline
          \bf English all & & & & & & \\
        \hline
        \bf Finnish WP & & & & & & \\
       \bf Finnish FSA & & & & & & \\
        \hline
        \bf North Sámi WP & & & & & & \\
        \bf North Sámi FSA& & & & & & \\
        \hline
        \bf Greenlandic WP & & & & & & \\
       \bf Greenlandic FSA & & & & & & \\
        \hline
    \end{tabular}
    \caption{The speed of spell-checking with different models. The first three
    figures are for average running text and the second with words that are not
    in the language model to begin with, i.e. only
    misspellings\label{table:language-speed}}
\end{table}

Here we show the effects of speed to optimisation methods.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \bf Rank: & $1^{st}$ & $2^{nd}$ & $3^{rd}$ & $4^{th}$ & $5^{th}$ & rest \\
        \bf Language and error models &  \% & \% & \% & \% & \% & \% \\
        \hline
        \bf English w/ 1 error & & & & & & \\
     \bf English w/ 2 errors & & & & & & \\
   \bf English w/ 3 errors & & & & & & \\
          \bf English w/ 4 errors & & & & & & \\
 \bf English /w 1 non-first error & & & & & & \\
 \bf English w/ 2 nonfirst errors & & & & & & \\
 \bf English w/ 4 nonfirst errors & & & & & & \\
        \hline
        \bf Finnish w/ 1 errors & & & & & & \\
        \bf Finnish w/ 2 errors & & & & & & \\
\bf Finnish w/ 1 nonfirst error & & & & & & \\
        \bf Finnish w/ 2 nonfirst errors & & & & & & \\
        \hline
        \bf North Sámi w/ 1 error & & & & & & \\
        \bf Nort Sámi w/ 2 errors & & & & & & \\
        \bf North Sámi w/ 1 nonfirst errors & & & & & & \\
        \bf North Sámi w/ 2 nonfirst errors & & & & & & \\
        \hline
        \bf Greenlandic w/ 1 error & & & & & & \\
       \bf Greenlandic w/ 2 errors & & & & & & \\
        \bf Greenlandic w/ 1 nonfirst error & & & & & & \\
       \bf Greenlandic w/ 2 nonfirst errors & & & & & & \\
        \hline
    \end{tabular}
    \caption{The effect of different optimisations to speed
    \label{table:optimisation-speed}}
\end{table}

In the next figure we show the plot of different optimisation in speed-quality
scale, on vertical axis the models covering the errors on horizontal axis the
quality in \% units.

\begin{figure}
    \centering
    \missingfigure{R-plot of Y:quality, X:ed1nf,ed1,ed2nf,ed2,... and language
    lines}
    \caption{A plot of dubious quality
    \label{fig:optimisation-speed-quality}}
\end{figure}

\subsection{Error analysis}
\label{subsec:error-analysis}

The errors were like so.


\section{Future Directions}
\label{sec:future}

In future.

\bibliography{fstspell2012}

\section*{Soundex automaton}
\label{appendix:soundex}

There is a commented AT\&T format automaton in our source code
repository\footnote{\url{}}. This description encodes fully the soundex
algorithm.  The format is following: on lines with four fields represent arcs,
the first field is beginning state of the arc and second field is the end state
of the arc. The third field is the input symbol of the arc and fourth field is
the output symbol of the arc. The rows with one field are the final states. The
resulting automaton is drawn in the figure~\ref{fig:soundex}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{graphicx/soundex}
    \caption{Soundex algorithm as finite-state automaton
    \label{fig:soundex}}
\end{figure}

\end{document} 

% vim: set spell:
